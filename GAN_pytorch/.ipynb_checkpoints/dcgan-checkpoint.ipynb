{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27e59407-07a8-471f-9421-99f661ef924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aa8c8f7-0eed-4c72-bcc8-4848840fb9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = True\n",
    "DATA_PATH = 'c:/Users/admin/Desktop/Dunhuang_dataset/'\n",
    "OUT_PATH = 'output_dunhuang_batchsize1024'\n",
    "LOG_FILE = os.path.join(OUT_PATH, 'log.txt')\n",
    "BATCH_SIZE = 1024\n",
    "IMAGE_CHANNEL = 3\n",
    "Z_DIM = 100\n",
    "G_HIDDEN = 64\n",
    "X_DIM = 64\n",
    "D_HIDDEN = 64\n",
    "EPOCH_NUM = 500\n",
    "REAL_LABEL = 1.\n",
    "FAKE_LABEL = 0.\n",
    "lr = 2e-4\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b36708c-a7ca-4431-8546-a193c6ebb0ba",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to output_dunhuang_batchsize1024\\log.txt\n",
      "\n",
      "PyTorch version: 1.13.1\n",
      "CUDA version: 11.7\n",
      "\n",
      "Random Seed:  1\n",
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Epoch 0 [0/3] loss_D_real: 0.9798 loss_D_fake: 0.8300 loss_G 4.8987\n",
      "Epoch 1 [0/3] loss_D_real: 0.7889 loss_D_fake: 0.4679 loss_G 5.3094\n",
      "Epoch 2 [0/3] loss_D_real: 0.8035 loss_D_fake: 0.5483 loss_G 7.1521\n",
      "Epoch 3 [0/3] loss_D_real: 0.3559 loss_D_fake: 0.8038 loss_G 10.4318\n",
      "Epoch 4 [0/3] loss_D_real: 0.6162 loss_D_fake: 0.0730 loss_G 8.7450\n",
      "Epoch 5 [0/3] loss_D_real: 0.2485 loss_D_fake: 1.3129 loss_G 12.7053\n",
      "Epoch 6 [0/3] loss_D_real: 0.5553 loss_D_fake: 0.0122 loss_G 11.1282\n",
      "Epoch 7 [0/3] loss_D_real: 0.2409 loss_D_fake: 0.1834 loss_G 9.7037\n",
      "Epoch 8 [0/3] loss_D_real: 0.2109 loss_D_fake: 0.8542 loss_G 15.2147\n",
      "Epoch 9 [0/3] loss_D_real: 0.1106 loss_D_fake: 3.4570 loss_G 18.2627\n",
      "Epoch 10 [0/3] loss_D_real: 0.1376 loss_D_fake: 0.0038 loss_G 6.7209\n",
      "Epoch 11 [0/3] loss_D_real: 0.4133 loss_D_fake: 0.0037 loss_G 8.2359\n",
      "Epoch 12 [0/3] loss_D_real: 0.5022 loss_D_fake: 0.0049 loss_G 6.9759\n",
      "Epoch 13 [0/3] loss_D_real: 0.3218 loss_D_fake: 0.0216 loss_G 5.0334\n",
      "Epoch 14 [0/3] loss_D_real: 0.3324 loss_D_fake: 0.3235 loss_G 5.0919\n",
      "Epoch 15 [0/3] loss_D_real: 0.4933 loss_D_fake: 0.7876 loss_G 6.3251\n",
      "Epoch 16 [0/3] loss_D_real: 0.5315 loss_D_fake: 0.2060 loss_G 4.8279\n",
      "Epoch 17 [0/3] loss_D_real: 0.3094 loss_D_fake: 1.0412 loss_G 6.8814\n",
      "Epoch 18 [0/3] loss_D_real: 0.3570 loss_D_fake: 0.3710 loss_G 4.7941\n",
      "Epoch 19 [0/3] loss_D_real: 1.7016 loss_D_fake: 0.0500 loss_G 2.7898\n",
      "Epoch 20 [0/3] loss_D_real: 0.5981 loss_D_fake: 0.1453 loss_G 3.0175\n",
      "Epoch 21 [0/3] loss_D_real: 0.3409 loss_D_fake: 0.6589 loss_G 4.5549\n",
      "Epoch 22 [0/3] loss_D_real: 1.3960 loss_D_fake: 0.0465 loss_G 2.5369\n",
      "Epoch 23 [0/3] loss_D_real: 0.5749 loss_D_fake: 0.4621 loss_G 4.3031\n",
      "Epoch 24 [0/3] loss_D_real: 1.4604 loss_D_fake: 0.0613 loss_G 2.7880\n",
      "Epoch 25 [0/3] loss_D_real: 0.6535 loss_D_fake: 0.3488 loss_G 3.3233\n",
      "Epoch 26 [0/3] loss_D_real: 0.5351 loss_D_fake: 0.4570 loss_G 4.1637\n",
      "Epoch 27 [0/3] loss_D_real: 0.5866 loss_D_fake: 0.1499 loss_G 1.9426\n",
      "Epoch 28 [0/3] loss_D_real: 0.8954 loss_D_fake: 0.4827 loss_G 0.9788\n",
      "Epoch 29 [0/3] loss_D_real: 0.5627 loss_D_fake: 0.2719 loss_G 2.5708\n",
      "Epoch 30 [0/3] loss_D_real: 0.4312 loss_D_fake: 0.4905 loss_G 3.3039\n",
      "Epoch 31 [0/3] loss_D_real: 0.6373 loss_D_fake: 0.3119 loss_G 2.5970\n",
      "Epoch 32 [0/3] loss_D_real: 0.3389 loss_D_fake: 0.4069 loss_G 3.7753\n",
      "Epoch 33 [0/3] loss_D_real: 0.9277 loss_D_fake: 0.3284 loss_G 2.7469\n",
      "Epoch 34 [0/3] loss_D_real: 0.2621 loss_D_fake: 0.3033 loss_G 3.5227\n",
      "Epoch 35 [0/3] loss_D_real: 0.3109 loss_D_fake: 0.4511 loss_G 4.0036\n",
      "Epoch 36 [0/3] loss_D_real: 0.3349 loss_D_fake: 0.3633 loss_G 4.6190\n",
      "Epoch 37 [0/3] loss_D_real: 0.2656 loss_D_fake: 0.1709 loss_G 4.3852\n",
      "Epoch 38 [0/3] loss_D_real: 0.2037 loss_D_fake: 0.2435 loss_G 5.0626\n",
      "Epoch 39 [0/3] loss_D_real: 0.2495 loss_D_fake: 0.1786 loss_G 5.3994\n",
      "Epoch 40 [0/3] loss_D_real: 0.2072 loss_D_fake: 0.4695 loss_G 6.2387\n",
      "Epoch 41 [0/3] loss_D_real: 0.2214 loss_D_fake: 0.2260 loss_G 5.1195\n",
      "Epoch 42 [0/3] loss_D_real: 0.1091 loss_D_fake: 0.0425 loss_G 4.1117\n",
      "Epoch 43 [0/3] loss_D_real: 0.3585 loss_D_fake: 0.2866 loss_G 1.9099\n",
      "Epoch 44 [0/3] loss_D_real: 0.3435 loss_D_fake: 0.1285 loss_G 3.3031\n",
      "Epoch 45 [0/3] loss_D_real: 0.3519 loss_D_fake: 0.2289 loss_G 4.4869\n",
      "Epoch 46 [0/3] loss_D_real: 0.1552 loss_D_fake: 0.2849 loss_G 4.9857\n",
      "Epoch 47 [0/3] loss_D_real: 0.1027 loss_D_fake: 0.2070 loss_G 4.8012\n",
      "Epoch 48 [0/3] loss_D_real: 0.1377 loss_D_fake: 0.2115 loss_G 5.2745\n",
      "Epoch 49 [0/3] loss_D_real: 0.2812 loss_D_fake: 0.0822 loss_G 4.8188\n",
      "Epoch 50 [0/3] loss_D_real: 0.2314 loss_D_fake: 0.4828 loss_G 8.6597\n",
      "Epoch 51 [0/3] loss_D_real: 0.8031 loss_D_fake: 0.0031 loss_G 10.1393\n",
      "Epoch 52 [0/3] loss_D_real: 0.8652 loss_D_fake: 0.0026 loss_G 11.3529\n",
      "Epoch 53 [0/3] loss_D_real: 0.5138 loss_D_fake: 0.0105 loss_G 8.4543\n",
      "Epoch 54 [0/3] loss_D_real: 0.4931 loss_D_fake: 0.0203 loss_G 7.2708\n",
      "Epoch 55 [0/3] loss_D_real: 0.8468 loss_D_fake: 0.0049 loss_G 8.2399\n",
      "Epoch 56 [0/3] loss_D_real: 0.2186 loss_D_fake: 0.1454 loss_G 5.9646\n",
      "Epoch 57 [0/3] loss_D_real: 0.5337 loss_D_fake: 0.1052 loss_G 4.3229\n",
      "Epoch 58 [0/3] loss_D_real: 0.1765 loss_D_fake: 0.1000 loss_G 4.3276\n",
      "Epoch 59 [0/3] loss_D_real: 0.1138 loss_D_fake: 0.1220 loss_G 4.6369\n",
      "Epoch 60 [0/3] loss_D_real: 0.1328 loss_D_fake: 0.1155 loss_G 5.0413\n",
      "Epoch 61 [0/3] loss_D_real: 0.2055 loss_D_fake: 0.3629 loss_G 7.2031\n",
      "Epoch 62 [0/3] loss_D_real: 0.5449 loss_D_fake: 0.0324 loss_G 5.5107\n",
      "Epoch 63 [0/3] loss_D_real: 0.2527 loss_D_fake: 0.0450 loss_G 6.1744\n",
      "Epoch 64 [0/3] loss_D_real: 0.2182 loss_D_fake: 0.0760 loss_G 6.0917\n",
      "Epoch 65 [0/3] loss_D_real: 0.2175 loss_D_fake: 0.0807 loss_G 6.3260\n",
      "Epoch 66 [0/3] loss_D_real: 0.2178 loss_D_fake: 0.1053 loss_G 5.4519\n",
      "Epoch 67 [0/3] loss_D_real: 0.2320 loss_D_fake: 0.2407 loss_G 7.7728\n",
      "Epoch 68 [0/3] loss_D_real: 0.4256 loss_D_fake: 0.0088 loss_G 6.7937\n",
      "Epoch 69 [0/3] loss_D_real: 0.2568 loss_D_fake: 0.0120 loss_G 7.5067\n",
      "Epoch 70 [0/3] loss_D_real: 0.7303 loss_D_fake: 0.0044 loss_G 7.4800\n",
      "Epoch 71 [0/3] loss_D_real: 0.1129 loss_D_fake: 0.1007 loss_G 5.8380\n",
      "Epoch 72 [0/3] loss_D_real: 0.1961 loss_D_fake: 0.1660 loss_G 5.8424\n",
      "Epoch 73 [0/3] loss_D_real: 0.3023 loss_D_fake: 0.1069 loss_G 5.1533\n",
      "Epoch 74 [0/3] loss_D_real: 0.1947 loss_D_fake: 0.1913 loss_G 5.4680\n",
      "Epoch 75 [0/3] loss_D_real: 0.2359 loss_D_fake: 0.1930 loss_G 6.0031\n",
      "Epoch 76 [0/3] loss_D_real: 0.1801 loss_D_fake: 0.1988 loss_G 6.3707\n",
      "Epoch 77 [0/3] loss_D_real: 0.3388 loss_D_fake: 0.1227 loss_G 6.0573\n",
      "Epoch 78 [0/3] loss_D_real: 0.1321 loss_D_fake: 0.2397 loss_G 5.3657\n",
      "Epoch 79 [0/3] loss_D_real: 0.1856 loss_D_fake: 0.2718 loss_G 5.3485\n",
      "Epoch 80 [0/3] loss_D_real: 1.7447 loss_D_fake: 0.0064 loss_G 0.9443\n",
      "Epoch 81 [0/3] loss_D_real: 0.3343 loss_D_fake: 0.0342 loss_G 2.9557\n",
      "Epoch 82 [0/3] loss_D_real: 0.2906 loss_D_fake: 0.0771 loss_G 3.8395\n",
      "Epoch 83 [0/3] loss_D_real: 0.2376 loss_D_fake: 0.1054 loss_G 3.8278\n",
      "Epoch 84 [0/3] loss_D_real: 0.2462 loss_D_fake: 0.2479 loss_G 4.3282\n",
      "Epoch 85 [0/3] loss_D_real: 0.7724 loss_D_fake: 0.1651 loss_G 3.3653\n",
      "Epoch 86 [0/3] loss_D_real: 0.1689 loss_D_fake: 0.4987 loss_G 5.3510\n",
      "Epoch 87 [0/3] loss_D_real: 0.1568 loss_D_fake: 0.5424 loss_G 6.5741\n",
      "Epoch 88 [0/3] loss_D_real: 2.8741 loss_D_fake: 0.0017 loss_G 4.5167\n",
      "Epoch 89 [0/3] loss_D_real: 0.6871 loss_D_fake: 0.0107 loss_G 6.4834\n",
      "Epoch 90 [0/3] loss_D_real: 0.8135 loss_D_fake: 0.0433 loss_G 3.7681\n",
      "Epoch 91 [0/3] loss_D_real: 0.3463 loss_D_fake: 0.2622 loss_G 4.1333\n",
      "Epoch 92 [0/3] loss_D_real: 0.0233 loss_D_fake: 2.3964 loss_G 7.1465\n",
      "Epoch 93 [0/3] loss_D_real: 0.6751 loss_D_fake: 0.0808 loss_G 3.4391\n",
      "Epoch 94 [0/3] loss_D_real: 0.5645 loss_D_fake: 0.1878 loss_G 2.5009\n",
      "Epoch 95 [0/3] loss_D_real: 0.2483 loss_D_fake: 0.7602 loss_G 5.2156\n",
      "Epoch 96 [0/3] loss_D_real: 0.4928 loss_D_fake: 0.0500 loss_G 2.8994\n",
      "Epoch 97 [0/3] loss_D_real: 0.1457 loss_D_fake: 0.7011 loss_G 4.9147\n",
      "Epoch 98 [0/3] loss_D_real: 0.2930 loss_D_fake: 0.3155 loss_G 4.1412\n",
      "Epoch 99 [0/3] loss_D_real: 0.3790 loss_D_fake: 0.1276 loss_G 3.7419\n",
      "Epoch 100 [0/3] loss_D_real: 0.1229 loss_D_fake: 0.1241 loss_G 4.4878\n",
      "Epoch 101 [0/3] loss_D_real: 0.1341 loss_D_fake: 0.2004 loss_G 5.0974\n",
      "Epoch 102 [0/3] loss_D_real: 0.4894 loss_D_fake: 0.0187 loss_G 4.2705\n",
      "Epoch 103 [0/3] loss_D_real: 0.0981 loss_D_fake: 0.1824 loss_G 5.5604\n",
      "Epoch 104 [0/3] loss_D_real: 0.3017 loss_D_fake: 0.0859 loss_G 3.7781\n",
      "Epoch 105 [0/3] loss_D_real: 0.0859 loss_D_fake: 0.2559 loss_G 4.6160\n",
      "Epoch 106 [0/3] loss_D_real: 0.1096 loss_D_fake: 0.0865 loss_G 5.2686\n",
      "Epoch 107 [0/3] loss_D_real: 0.0869 loss_D_fake: 0.1746 loss_G 5.2164\n",
      "Epoch 108 [0/3] loss_D_real: 0.3241 loss_D_fake: 0.1156 loss_G 4.6702\n",
      "Epoch 109 [0/3] loss_D_real: 0.0369 loss_D_fake: 2.1676 loss_G 9.5031\n",
      "Epoch 110 [0/3] loss_D_real: 0.1212 loss_D_fake: 0.1857 loss_G 3.5045\n",
      "Epoch 111 [0/3] loss_D_real: 0.2148 loss_D_fake: 0.1077 loss_G 4.1337\n",
      "Epoch 112 [0/3] loss_D_real: 0.0923 loss_D_fake: 0.0950 loss_G 4.7018\n",
      "Epoch 113 [0/3] loss_D_real: 0.1824 loss_D_fake: 0.2266 loss_G 5.5392\n",
      "Epoch 114 [0/3] loss_D_real: 0.4050 loss_D_fake: 0.2637 loss_G 4.7119\n",
      "Epoch 115 [0/3] loss_D_real: 0.0855 loss_D_fake: 2.3556 loss_G 8.3412\n",
      "Epoch 116 [0/3] loss_D_real: 0.1648 loss_D_fake: 0.7919 loss_G 6.2346\n",
      "Epoch 117 [0/3] loss_D_real: 0.1484 loss_D_fake: 0.5366 loss_G 6.7561\n",
      "Epoch 118 [0/3] loss_D_real: 0.8204 loss_D_fake: 0.0567 loss_G 4.0999\n",
      "Epoch 119 [0/3] loss_D_real: 0.3824 loss_D_fake: 0.2524 loss_G 4.1755\n",
      "Epoch 120 [0/3] loss_D_real: 0.1138 loss_D_fake: 1.5768 loss_G 8.4384\n",
      "Epoch 121 [0/3] loss_D_real: 0.0544 loss_D_fake: 1.3122 loss_G 5.9700\n",
      "Epoch 122 [0/3] loss_D_real: 0.0979 loss_D_fake: 0.6166 loss_G 4.5887\n",
      "Epoch 123 [0/3] loss_D_real: 0.2238 loss_D_fake: 0.6072 loss_G 5.0872\n",
      "Epoch 124 [0/3] loss_D_real: 0.4842 loss_D_fake: 0.2036 loss_G 3.9755\n",
      "Epoch 125 [0/3] loss_D_real: 0.1884 loss_D_fake: 0.3769 loss_G 5.3020\n",
      "Epoch 126 [0/3] loss_D_real: 0.4616 loss_D_fake: 0.1079 loss_G 3.3277\n",
      "Epoch 127 [0/3] loss_D_real: 0.0936 loss_D_fake: 1.0224 loss_G 6.1947\n",
      "Epoch 128 [0/3] loss_D_real: 0.3393 loss_D_fake: 0.7512 loss_G 6.6775\n",
      "Epoch 129 [0/3] loss_D_real: 0.6280 loss_D_fake: 0.1207 loss_G 4.5244\n",
      "Epoch 130 [0/3] loss_D_real: 0.3847 loss_D_fake: 0.0812 loss_G 4.1846\n",
      "Epoch 131 [0/3] loss_D_real: 0.1562 loss_D_fake: 0.2818 loss_G 4.6053\n",
      "Epoch 132 [0/3] loss_D_real: 0.1882 loss_D_fake: 0.1780 loss_G 4.1725\n",
      "Epoch 133 [0/3] loss_D_real: 0.1715 loss_D_fake: 0.2777 loss_G 4.8421\n",
      "Epoch 134 [0/3] loss_D_real: 0.6826 loss_D_fake: 0.0418 loss_G 3.0779\n",
      "Epoch 135 [0/3] loss_D_real: 0.1320 loss_D_fake: 0.1828 loss_G 3.3706\n",
      "Epoch 136 [0/3] loss_D_real: 0.2611 loss_D_fake: 0.2467 loss_G 3.0436\n",
      "Epoch 137 [0/3] loss_D_real: 0.2477 loss_D_fake: 0.4678 loss_G 3.6256\n",
      "Epoch 138 [0/3] loss_D_real: 0.2107 loss_D_fake: 0.2607 loss_G 3.7503\n",
      "Epoch 139 [0/3] loss_D_real: 0.2055 loss_D_fake: 0.1961 loss_G 4.0960\n",
      "Epoch 140 [0/3] loss_D_real: 0.0981 loss_D_fake: 0.1279 loss_G 4.3891\n",
      "Epoch 141 [0/3] loss_D_real: 0.0963 loss_D_fake: 0.2013 loss_G 5.2959\n",
      "Epoch 142 [0/3] loss_D_real: 0.2058 loss_D_fake: 0.2651 loss_G 5.4066\n",
      "Epoch 143 [0/3] loss_D_real: 1.0170 loss_D_fake: 0.0519 loss_G 1.6572\n",
      "Epoch 144 [0/3] loss_D_real: 0.2031 loss_D_fake: 0.1956 loss_G 2.4134\n",
      "Epoch 145 [0/3] loss_D_real: 0.3066 loss_D_fake: 0.0286 loss_G 3.9076\n",
      "Epoch 146 [0/3] loss_D_real: 0.1853 loss_D_fake: 0.1189 loss_G 4.5743\n",
      "Epoch 147 [0/3] loss_D_real: 0.1951 loss_D_fake: 0.2731 loss_G 4.2624\n",
      "Epoch 148 [0/3] loss_D_real: 0.1952 loss_D_fake: 0.1412 loss_G 3.6719\n",
      "Epoch 149 [0/3] loss_D_real: 0.1378 loss_D_fake: 0.1199 loss_G 3.9276\n",
      "Epoch 150 [0/3] loss_D_real: 0.2782 loss_D_fake: 0.0854 loss_G 4.1313\n",
      "Epoch 151 [0/3] loss_D_real: 0.1510 loss_D_fake: 0.2142 loss_G 4.8568\n",
      "Epoch 152 [0/3] loss_D_real: 0.1265 loss_D_fake: 0.3179 loss_G 6.2847\n",
      "Epoch 153 [0/3] loss_D_real: 0.2514 loss_D_fake: 0.1084 loss_G 5.0100\n",
      "Epoch 154 [0/3] loss_D_real: 0.3119 loss_D_fake: 0.0739 loss_G 4.1380\n",
      "Epoch 155 [0/3] loss_D_real: 0.1080 loss_D_fake: 0.1603 loss_G 4.1306\n",
      "Epoch 156 [0/3] loss_D_real: 0.1565 loss_D_fake: 0.0580 loss_G 3.9750\n",
      "Epoch 157 [0/3] loss_D_real: 0.1428 loss_D_fake: 0.0982 loss_G 4.1047\n",
      "Epoch 158 [0/3] loss_D_real: 0.8331 loss_D_fake: 0.0198 loss_G 2.6215\n",
      "Epoch 159 [0/3] loss_D_real: 0.1524 loss_D_fake: 0.0568 loss_G 3.7018\n",
      "Epoch 160 [0/3] loss_D_real: 0.1216 loss_D_fake: 0.1231 loss_G 4.2876\n",
      "Epoch 161 [0/3] loss_D_real: 0.2207 loss_D_fake: 0.0759 loss_G 3.4408\n",
      "Epoch 162 [0/3] loss_D_real: 0.0156 loss_D_fake: 1.0127 loss_G 7.3515\n",
      "Epoch 163 [0/3] loss_D_real: 0.0327 loss_D_fake: 0.0885 loss_G 3.5153\n",
      "Epoch 164 [0/3] loss_D_real: 0.1613 loss_D_fake: 0.0991 loss_G 3.7249\n",
      "Epoch 165 [0/3] loss_D_real: 0.0650 loss_D_fake: 1.4967 loss_G 9.9926\n",
      "Epoch 166 [0/3] loss_D_real: 0.5245 loss_D_fake: 0.0168 loss_G 5.6407\n",
      "Epoch 167 [0/3] loss_D_real: 1.1977 loss_D_fake: 0.0222 loss_G 4.1113\n",
      "Epoch 168 [0/3] loss_D_real: 0.4659 loss_D_fake: 0.2651 loss_G 3.6306\n",
      "Epoch 169 [0/3] loss_D_real: 0.1627 loss_D_fake: 0.2272 loss_G 4.4601\n",
      "Epoch 170 [0/3] loss_D_real: 0.5744 loss_D_fake: 0.0602 loss_G 2.8692\n",
      "Epoch 171 [0/3] loss_D_real: 0.1466 loss_D_fake: 0.2225 loss_G 3.2784\n",
      "Epoch 172 [0/3] loss_D_real: 0.2208 loss_D_fake: 0.4494 loss_G 4.6026\n",
      "Epoch 173 [0/3] loss_D_real: 0.2333 loss_D_fake: 0.6637 loss_G 6.3753\n",
      "Epoch 174 [0/3] loss_D_real: 0.0830 loss_D_fake: 0.3205 loss_G 5.2987\n",
      "Epoch 175 [0/3] loss_D_real: 0.0922 loss_D_fake: 0.3163 loss_G 4.9523\n",
      "Epoch 176 [0/3] loss_D_real: 0.1377 loss_D_fake: 0.1465 loss_G 4.5207\n",
      "Epoch 177 [0/3] loss_D_real: 0.1491 loss_D_fake: 0.2226 loss_G 5.6903\n",
      "Epoch 178 [0/3] loss_D_real: 0.5547 loss_D_fake: 0.0425 loss_G 3.6136\n",
      "Epoch 179 [0/3] loss_D_real: 0.0705 loss_D_fake: 0.1838 loss_G 2.9519\n",
      "Epoch 180 [0/3] loss_D_real: 0.1752 loss_D_fake: 0.0427 loss_G 3.9917\n",
      "Epoch 181 [0/3] loss_D_real: 0.1752 loss_D_fake: 0.0763 loss_G 4.0244\n",
      "Epoch 182 [0/3] loss_D_real: 0.1712 loss_D_fake: 0.1841 loss_G 4.4728\n",
      "Epoch 183 [0/3] loss_D_real: 0.6086 loss_D_fake: 0.0806 loss_G 2.0402\n",
      "Epoch 184 [0/3] loss_D_real: 1.0786 loss_D_fake: 0.0076 loss_G 1.8681\n",
      "Epoch 185 [0/3] loss_D_real: 0.1890 loss_D_fake: 0.0520 loss_G 4.6513\n",
      "Epoch 186 [0/3] loss_D_real: 0.1647 loss_D_fake: 0.4543 loss_G 4.1826\n",
      "Epoch 187 [0/3] loss_D_real: 0.5873 loss_D_fake: 0.3069 loss_G 3.5135\n",
      "Epoch 188 [0/3] loss_D_real: 0.2110 loss_D_fake: 0.2723 loss_G 4.3575\n",
      "Epoch 189 [0/3] loss_D_real: 0.2118 loss_D_fake: 0.2113 loss_G 4.8951\n",
      "Epoch 190 [0/3] loss_D_real: 0.1842 loss_D_fake: 0.1306 loss_G 4.9682\n",
      "Epoch 191 [0/3] loss_D_real: 0.1325 loss_D_fake: 0.1604 loss_G 6.0412\n",
      "Epoch 192 [0/3] loss_D_real: 0.2151 loss_D_fake: 0.0543 loss_G 5.5290\n",
      "Epoch 193 [0/3] loss_D_real: 0.1780 loss_D_fake: 1.0090 loss_G 9.9855\n",
      "Epoch 194 [0/3] loss_D_real: 0.0885 loss_D_fake: 1.1948 loss_G 8.7164\n",
      "Epoch 195 [0/3] loss_D_real: 0.0453 loss_D_fake: 0.8175 loss_G 6.3560\n",
      "Epoch 196 [0/3] loss_D_real: 0.1025 loss_D_fake: 0.2537 loss_G 4.6289\n",
      "Epoch 197 [0/3] loss_D_real: 0.1145 loss_D_fake: 0.1261 loss_G 4.4172\n",
      "Epoch 198 [0/3] loss_D_real: 0.1408 loss_D_fake: 0.0777 loss_G 4.4373\n",
      "Epoch 199 [0/3] loss_D_real: 0.1248 loss_D_fake: 0.1397 loss_G 4.8584\n",
      "Epoch 200 [0/3] loss_D_real: 0.1918 loss_D_fake: 0.2661 loss_G 6.1202\n",
      "Epoch 201 [0/3] loss_D_real: 0.7062 loss_D_fake: 0.0129 loss_G 4.3939\n",
      "Epoch 202 [0/3] loss_D_real: 0.3266 loss_D_fake: 0.0538 loss_G 4.6526\n",
      "Epoch 203 [0/3] loss_D_real: 0.3419 loss_D_fake: 0.0470 loss_G 4.2568\n",
      "Epoch 204 [0/3] loss_D_real: 0.3025 loss_D_fake: 0.0438 loss_G 4.2614\n",
      "Epoch 205 [0/3] loss_D_real: 0.8046 loss_D_fake: 0.0188 loss_G 4.1406\n",
      "Epoch 206 [0/3] loss_D_real: 0.3354 loss_D_fake: 0.2087 loss_G 4.2430\n",
      "Epoch 207 [0/3] loss_D_real: 0.2043 loss_D_fake: 0.9931 loss_G 7.1589\n",
      "Epoch 208 [0/3] loss_D_real: 0.2186 loss_D_fake: 0.0475 loss_G 5.2129\n",
      "Epoch 209 [0/3] loss_D_real: 0.1318 loss_D_fake: 0.1712 loss_G 5.1531\n",
      "Epoch 210 [0/3] loss_D_real: 0.5012 loss_D_fake: 0.1859 loss_G 4.1892\n",
      "Epoch 211 [0/3] loss_D_real: 0.0692 loss_D_fake: 0.4197 loss_G 5.2722\n",
      "Epoch 212 [0/3] loss_D_real: 0.1706 loss_D_fake: 0.8406 loss_G 8.5391\n",
      "Epoch 213 [0/3] loss_D_real: 0.0306 loss_D_fake: 0.4265 loss_G 6.3549\n",
      "Epoch 214 [0/3] loss_D_real: 0.1177 loss_D_fake: 0.5352 loss_G 6.6518\n",
      "Epoch 215 [0/3] loss_D_real: 0.3253 loss_D_fake: 0.1156 loss_G 4.2543\n",
      "Epoch 216 [0/3] loss_D_real: 0.1825 loss_D_fake: 0.3190 loss_G 4.9759\n",
      "Epoch 217 [0/3] loss_D_real: 0.5935 loss_D_fake: 0.0454 loss_G 3.1936\n",
      "Epoch 218 [0/3] loss_D_real: 0.1780 loss_D_fake: 0.0805 loss_G 3.2372\n",
      "Epoch 219 [0/3] loss_D_real: 0.2005 loss_D_fake: 0.0231 loss_G 3.6728\n",
      "Epoch 220 [0/3] loss_D_real: 0.2511 loss_D_fake: 0.0825 loss_G 3.3881\n",
      "Epoch 221 [0/3] loss_D_real: 0.1691 loss_D_fake: 0.0830 loss_G 3.1853\n",
      "Epoch 222 [0/3] loss_D_real: 0.2040 loss_D_fake: 0.0719 loss_G 2.8948\n",
      "Epoch 223 [0/3] loss_D_real: 0.1322 loss_D_fake: 0.0749 loss_G 3.2508\n",
      "Epoch 224 [0/3] loss_D_real: 0.1286 loss_D_fake: 0.1951 loss_G 2.8970\n",
      "Epoch 225 [0/3] loss_D_real: 0.2746 loss_D_fake: 0.0577 loss_G 2.9405\n",
      "Epoch 226 [0/3] loss_D_real: 0.2642 loss_D_fake: 0.1215 loss_G 3.4547\n",
      "Epoch 227 [0/3] loss_D_real: 0.2164 loss_D_fake: 0.1300 loss_G 3.6854\n",
      "Epoch 228 [0/3] loss_D_real: 0.1380 loss_D_fake: 0.2077 loss_G 4.4519\n",
      "Epoch 229 [0/3] loss_D_real: 0.1314 loss_D_fake: 0.1736 loss_G 4.2874\n",
      "Epoch 230 [0/3] loss_D_real: 0.1392 loss_D_fake: 0.1599 loss_G 4.5072\n",
      "Epoch 231 [0/3] loss_D_real: 0.1375 loss_D_fake: 0.2071 loss_G 4.9995\n",
      "Epoch 232 [0/3] loss_D_real: 0.1417 loss_D_fake: 0.1695 loss_G 5.0250\n",
      "Epoch 233 [0/3] loss_D_real: 0.2530 loss_D_fake: 0.1460 loss_G 4.6869\n",
      "Epoch 234 [0/3] loss_D_real: 0.1354 loss_D_fake: 0.5100 loss_G 8.0534\n",
      "Epoch 235 [0/3] loss_D_real: 0.0919 loss_D_fake: 0.6308 loss_G 9.4996\n",
      "Epoch 236 [0/3] loss_D_real: 0.0533 loss_D_fake: 1.0975 loss_G 10.6341\n",
      "Epoch 237 [0/3] loss_D_real: 0.0235 loss_D_fake: 0.7380 loss_G 7.0870\n",
      "Epoch 238 [0/3] loss_D_real: 0.1180 loss_D_fake: 0.5076 loss_G 7.5530\n",
      "Epoch 239 [0/3] loss_D_real: 0.1222 loss_D_fake: 0.3744 loss_G 6.7713\n",
      "Epoch 240 [0/3] loss_D_real: 0.3585 loss_D_fake: 0.0759 loss_G 4.6048\n"
     ]
    }
   ],
   "source": [
    "utils.clear_folder(OUT_PATH)\n",
    "print('Logging to {}\\n'.format(LOG_FILE))\n",
    "sys.stdout = utils.StdOut(LOG_FILE)\n",
    "CUDA = CUDA and torch.cuda.is_available()\n",
    "print('PyTorch version: {}'.format(torch.__version__))\n",
    "if CUDA:\n",
    "    print('CUDA version: {}\\n'.format(torch.version.cuda))\n",
    "if seed is None:\n",
    "    seed = np.random.randint(1, 10000)\n",
    "print('Random Seed: ', seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "cudnn.benchmark = True\n",
    "device = torch.device('cuda:0' if CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47735c35-59f2-4684-b4fe-0fa109946e59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # 1st layer\n",
    "            nn.ConvTranspose2d(Z_DIM, G_HIDDEN*8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN*8),\n",
    "            nn.ReLU(True),\n",
    "            # 2nd layer\n",
    "            nn.ConvTranspose2d(G_HIDDEN*8, G_HIDDEN*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN*4),\n",
    "            nn.ReLU(True),\n",
    "            # 3rd layer\n",
    "            nn.ConvTranspose2d(G_HIDDEN*4, G_HIDDEN*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN*2),\n",
    "            nn.ReLU(True),\n",
    "            # 4th layer\n",
    "            nn.ConvTranspose2d(G_HIDDEN*2, G_HIDDEN, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN),\n",
    "            nn.ReLU(True),\n",
    "            # output layer\n",
    "            nn.ConvTranspose2d(G_HIDDEN, IMAGE_CHANNEL, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e30da00f-333f-4904-babe-afb37e5e5c55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9a3b4c7-5ebf-432f-a467-09717fce7806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "netG = Generator().to(device)\n",
    "netG.apply(weights_init)\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afb813b2-ad63-47d5-b6f4-f8e941670642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # 1st layer\n",
    "            nn.Conv2d(IMAGE_CHANNEL, D_HIDDEN, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 2nd layer\n",
    "            nn.Conv2d(D_HIDDEN, D_HIDDEN*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(D_HIDDEN*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 3rd layer\n",
    "            nn.Conv2d(D_HIDDEN*2, D_HIDDEN*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(D_HIDDEN*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 4th layer\n",
    "            nn.Conv2d(D_HIDDEN*4, D_HIDDEN*8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(D_HIDDEN*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # output layer\n",
    "            nn.Conv2d(D_HIDDEN*8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.main(input).view(-1,1).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9276cc7f-a0d3-41be-938b-34fae1e7e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "netD = Discriminator().to(device)\n",
    "netD.apply(weights_init)\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e29f9843-25df-4210-bc3d-873a04e0d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82d7baef-1b4f-4def-a11f-1d96c5b5d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dset.ImageFolder(root=DATA_PATH,\n",
    "                     transform=transforms.Compose([\n",
    "                         transforms.RandomResizedCrop(X_DIM),\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                     ]))\n",
    "\n",
    "assert dataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcf414ee-3685-4a56-897c-9991344b5ec5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m viz_noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(BATCH_SIZE, Z_DIM, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCH_NUM):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      4\u001b[0m         x_real \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      5\u001b[0m         real_label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((x_real\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m),),REAL_LABEL, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:435\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:381\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1034\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1027\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1034\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "viz_noise = torch.randn(BATCH_SIZE, Z_DIM, 1, 1, device=device)\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    for i, data in enumerate(dataloader):\n",
    "        x_real = data[0].to(device)\n",
    "        real_label = torch.full((x_real.size(0),),REAL_LABEL, device=device)\n",
    "        fake_label = torch.full((x_real.size(0),),FAKE_LABEL, device=device)\n",
    "        \n",
    "        # Update D with real data\n",
    "        netD.zero_grad()\n",
    "        y_real = netD(x_real)\n",
    "        loss_D_real = criterion(y_real, real_label)\n",
    "        loss_D_real.backward()\n",
    "        \n",
    "        # Update D with fake data\n",
    "        z_noise = torch.randn(x_real.size(0), Z_DIM, 1, 1, device=device)\n",
    "        x_fake = netG(z_noise)\n",
    "        y_fake = netD(x_fake.detach())\n",
    "        loss_D_fake = criterion(y_fake, fake_label)\n",
    "        loss_D_fake.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # Update G with fake data\n",
    "        netG.zero_grad()\n",
    "        y_fake_r = netD(x_fake)\n",
    "        loss_G = criterion(y_fake_r, real_label)\n",
    "        loss_G.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        if i%100==0:\n",
    "            print('Epoch {} [{}/{}] loss_D_real: {:.4f} loss_D_fake: {:.4f} loss_G {:.4f}'.format(\n",
    "                epoch, i, len(dataloader), loss_D_real.mean().item(), loss_D_fake.mean().item(), loss_G.mean().item()))\n",
    "            vutils.save_image(x_real, os.path.join(OUT_PATH, 'real_samples.png'), normalize=True)\n",
    "            with torch.no_grad():\n",
    "                viz_sample = netG(viz_noise)\n",
    "                vutils.save_image(viz_sample, os.path.join(OUT_PATH, 'fake_samples_{}.png'.format(epoch)), normalize=True)\n",
    "            torch.save(netG.state_dict(), os.path.join(OUT_PATH, 'netG_{}.pth'.format(epoch)))\n",
    "            torch.save(netD.state_dict(), os.path.join(OUT_PATH, 'netD_{}.pth'.format(epoch)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
