{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3692c2-9259-49db-81cf-18bb3c024feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398712c4-542e-44b9-9532-9c6209fd90ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'c:/Users/admin/Documents/Datasets/cursive/'\n",
    "\n",
    "train_images = tf.keras.utils.image_dataset_from_directory(path, image_size=(256, 256), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0a4708-3590-4071-b4df-3307fe7834e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d065a4e4-cd14-4216-b863-959d6df5bd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_images.take(1):\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8df912-e344-4f59-ae54-b67089134813",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_images.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f62566-4ae6-46c3-ba15-9226f0649a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = layers.Rescaling(1./255)\n",
    "normalized_train_images = train_images.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_train_images))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d684f842-8cc3-4d93-8eb0-c4e30077dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (256, 256, 3)\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a01c07-3d34-4ccf-9fe7-364c7e868001",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class WGAN_GP():\n",
    "    def __init__(self, input_shape):\n",
    "\n",
    "        self.z_dim = 64\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "        self.loss_critic = {}\n",
    "        self.loss_gp = {}\n",
    "        self.loss_generator = {}\n",
    "        \n",
    "        # critic\n",
    "        self.n_critic = 5\n",
    "        self.penalty_const = 10\n",
    "        self.critic = self.build_critic()\n",
    "        self.critic.trainable = False\n",
    "\n",
    "        self.optimizer_critic = Adam(1e-4, 0.5, 0.9)\n",
    "\n",
    "        # build generator pipeline with frozen critic\n",
    "        self.generator = self.build_generator()\n",
    "        critic_output = self.critic(self.generator.output)\n",
    "        self.model = Model(self.generator.input, critic_output)\n",
    "        self.model.compile(loss = self.wasserstein_loss,\n",
    "                           optimizer = Adam(1e-4, 0.5, 0.9))\n",
    "        self.critic.trainable = True\n",
    "        \n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "\n",
    "        w_loss = -tf.reduce_mean(y_true*y_pred)\n",
    "\n",
    "        return w_loss\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        DIM = 64\n",
    "        model = tf.keras.Sequential(name='Generator') \n",
    "\n",
    "        model.add(layers.Input(shape=[self.z_dim])) \n",
    "\n",
    "        model.add(layers.Dense(8*8*8*DIM))                                                \n",
    "        model.add(layers.LayerNormalization()) \n",
    "        model.add(layers.ReLU())\n",
    "        model.add(layers.Reshape((8,8,8*DIM)))                                            \n",
    "        \n",
    "        model.add(layers.UpSampling2D((2,2), interpolation=\"bilinear\"))                 \n",
    "        model.add(layers.Conv2D(8*DIM, 5, padding='same'))                                  \n",
    "        model.add(layers.LayerNormalization()) \n",
    "        model.add(layers.ReLU())\n",
    "        \n",
    "        model.add(layers.UpSampling2D((2,2), interpolation=\"bilinear\"))               \n",
    "        model.add(layers.Conv2D(4*DIM, 5, padding='same'))                                \n",
    "        model.add(layers.LayerNormalization()) \n",
    "        model.add(layers.ReLU())\n",
    "        \n",
    "        model.add(layers.UpSampling2D((2,2), interpolation=\"bilinear\"))                    \n",
    "        model.add(layers.Conv2D(2*DIM, 5, padding='same'))                                 \n",
    "        model.add(layers.LayerNormalization()) \n",
    "        model.add(layers.ReLU())\n",
    "        \n",
    "        model.add(layers.UpSampling2D((2,2), interpolation=\"bilinear\"))                 \n",
    "        model.add(layers.Conv2D(DIM, 5, padding='same'))                              \n",
    "        model.add(layers.LayerNormalization()) \n",
    "        model.add(layers.ReLU())\n",
    "\n",
    "        model.add(layers.UpSampling2D((2,2), interpolation=\"bilinear\"))                   \n",
    "        model.add(layers.Conv2D(image_shape[-1], 5, padding='same', activation='tanh'))   \n",
    "\n",
    "        return model             \n",
    "    \n",
    "    def build_critic(self):\n",
    "\n",
    "        DIM = 64\n",
    "        model = tf.keras.Sequential(name='critics') \n",
    "\n",
    "        model.add(layers.Input(shape=self.input_shape)) \n",
    "\n",
    "        model.add(layers.Conv2D(1*DIM, 5, strides=2, padding='same', use_bias=False))    \n",
    "        model.add(layers.LeakyReLU(0.2))\n",
    "\n",
    "        model.add(layers.Conv2D(2*DIM, 5, strides=2, padding='same', use_bias=False))\n",
    "        model.add(layers.LeakyReLU(0.2))\n",
    "\n",
    "        model.add(layers.Conv2D(4*DIM, 5, strides=2, padding='same', use_bias=False))\n",
    "        model.add(layers.LeakyReLU(0.2))\n",
    "\n",
    "        model.add(layers.Conv2D(8*DIM, 5, strides=2, padding='same', use_bias=False))\n",
    "        model.add(layers.LeakyReLU(0.2))\n",
    "        \n",
    "        model.add(layers.Conv2D(16*DIM, 5, strides=2, padding='same', use_bias=False))\n",
    "        model.add(layers.LeakyReLU(0.2))\n",
    "        \n",
    "        model.add(layers.Conv2D(32*DIM, 5, strides=2, padding='same', use_bias=False))\n",
    "        model.add(layers.LeakyReLU(0.2))\n",
    "     \n",
    "        \n",
    "        model.add(layers.Flatten()) \n",
    "        model.add(layers.Dense(1)) \n",
    "\n",
    "        return model     \n",
    "    \n",
    " \n",
    "    def gradient_loss(self, grad):\n",
    "\n",
    "        loss = tf.square(grad)\n",
    "        loss = tf.reduce_sum(loss, axis=np.arange(1,len(loss.shape)))\n",
    "        loss = tf.sqrt(loss)\n",
    "        loss = tf.reduce_mean(tf.square(loss - 1))\n",
    "        loss = self.penalty_const * loss\n",
    "        return loss\n",
    "\n",
    "    def train_critic(self, real_images, batch_size):\n",
    "        real_labels = tf.ones(batch_size)\n",
    "        fake_labels = -tf.ones(batch_size)\n",
    "                  \n",
    "        g_input = tf.random.normal((batch_size, self.z_dim))\n",
    "        fake_images = self.generator.predict(g_input, verbose=0)\n",
    "        \n",
    "        with tf.GradientTape() as gradient_tape,\\\n",
    "             tf.GradientTape() as total_tape:\n",
    "            \n",
    "            # forward pass\n",
    "            pred_fake = self.critic(fake_images)\n",
    "            pred_real = self.critic(real_images)\n",
    "            \n",
    "            # calculate losses\n",
    "            loss_fake = self.wasserstein_loss(fake_labels, pred_fake)\n",
    "            loss_real = self.wasserstein_loss(real_labels, pred_real)           \n",
    "            \n",
    "            # gradient penalty      \n",
    "            epsilon = tf.random.uniform((batch_size,1,1,1))\n",
    "            interpolates = epsilon*real_images + (1-epsilon)*fake_images\n",
    "            gradient_tape.watch(interpolates)\n",
    "            \n",
    "            critic_interpolates = self.critic(interpolates)\n",
    "            gradients_interpolates = gradient_tape.gradient(critic_interpolates, [interpolates])\n",
    "            gradient_penalty = self.gradient_loss(gradients_interpolates)\n",
    "     \n",
    "            # total loss\n",
    "            total_loss = loss_fake + loss_real + gradient_penalty\n",
    "            \n",
    "            # apply gradients\n",
    "            gradients = total_tape.gradient(total_loss, self.critic.trainable_variables)\n",
    "            \n",
    "            self.optimizer_critic.apply_gradients(zip(gradients, self.critic.trainable_variables))\n",
    "\n",
    "        return loss_fake, loss_real, gradient_penalty\n",
    "                                                \n",
    "    def train(self, data_generator, batch_size, steps, interval=100):\n",
    "\n",
    "        val_g_input = tf.random.normal((batch_size, self.z_dim))\n",
    "        real_labels = tf.ones(batch_size)\n",
    "\n",
    "        for i in range(steps):\n",
    "            for _ in range(self.n_critic):\n",
    "                real_images = data_generator\n",
    "                loss_fake, loss_real, gradient_penalty = self.train_critic(real_images, batch_size)\n",
    "                critic_loss = loss_fake + loss_real\n",
    "            # train generator\n",
    "            g_input = tf.random.normal((batch_size, self.z_dim))\n",
    "            g_loss = self.model.train_on_batch(g_input, real_labels)\n",
    "            self.loss_gp[i] = gradient_penalty\n",
    "            self.loss_critic[i] = critic_loss.numpy()\n",
    "            self.loss_generator[i] = g_loss\n",
    "            if i%interval == 0:\n",
    "                msg = \"Step {}: g_loss {:.4f} critic_loss {:.4f} critic fake {:.4f}  critic_real {:.4f} penalty {:.4f}\"\\\n",
    "                .format(i, g_loss, critic_loss, loss_fake, loss_real, gradient_penalty)\n",
    "                print(msg)\n",
    "\n",
    "                fake_images = self.generator.predict(val_g_input, verbose=0)\n",
    "                self.plot_images(fake_images)\n",
    "                # self.plot_losses()\n",
    "\n",
    "    def plot_images(self, images):   \n",
    "        grid_row = 1\n",
    "        grid_col = 6\n",
    "        f, axarr = plt.subplots(grid_row, grid_col, figsize=(grid_col*3.5, grid_row*3.5))\n",
    "        for row in range(grid_row):\n",
    "            for col in range(grid_col):\n",
    "                if self.input_shape[-1]==1:\n",
    "                    axarr[col].imshow(images[col,:,:,0]*0.5+0.5, cmap='gray')\n",
    "                else:\n",
    "                    axarr[col].imshow(images[col]*0.5+0.5)\n",
    "                axarr[col].axis('off') \n",
    "        plt.show()\n",
    "\n",
    "    def plot_losses(self):\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, sharex=True)\n",
    "        fig.set_figwidth(10)\n",
    "        fig.set_figheight(6)\n",
    "        ax1.plot(list(self.loss_critic.values()), label='Critic loss', alpha=0.7)\n",
    "        ax1.set_title(\"Critic loss\")\n",
    "        ax2.plot(list(self.loss_generator.values()), label='Generator loss', alpha=0.7)\n",
    "        ax2.set_title(\"Generator loss\")\n",
    "        ax3.plot(list(self.loss_gp.values()), label='Gradient penalty', alpha=0.7)\n",
    "        ax3.set_title(\"Gradient penalty\")\n",
    "        plt.xlabel('Steps')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fabb2ee-ef4d-4cf3-a7e6-1a7e23f9fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan = WGAN_GP(image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2020608-a892-473b-b431-d6b3339adaac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wgan.train(image_batch, 16, 50000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275eaf91-b132-4c58-98f6-030f3dabdeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wgan.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1a77ed-53d2-47d0-8867-67d7fdae3e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wgan.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856354a7-0813-4a17-9108-75ddebde93d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wgan.critic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ebfcef-5ce1-431a-ba3a-a2be6a34fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tf.random.normal((8, 64))\n",
    "generated_images = wgan.generator.predict(z)\n",
    "wgan.plot_images(generated_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d8315-ca45-4a16-a45f-b6bd2269bc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    z = tf.random.normal((20, 64))\n",
    "    images = wgan.generator.predict(z, verbose=0)\n",
    "    \n",
    "    grid_row = 1\n",
    "    grid_col = 20\n",
    "    f, axarr = plt.subplots(grid_row, grid_col, figsize=(grid_col*10, grid_row*10))\n",
    "    for row in range(grid_row):\n",
    "        for col in range(grid_col):\n",
    "            axarr[col].imshow(images[col]*0.5+0.5)\n",
    "            axarr[col].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e3b0c9-6ec3-4ed1-812e-3e4bb3b6356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan.generator.save_weights('wgan_gp_cursive_.weights')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
