{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c96efbe-aedc-4f00-8e37-2eedb891960e",
   "metadata": {},
   "source": [
    "For whatever reason this just doesn't seem to work. All I can get is noise or total black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cec17e41-3f51-4bba-b3b2-698c9f95326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from enum import Enum\n",
    "from glob import glob\n",
    "from functools import partial\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow_addons.layers import InstanceNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ceb3931-c815-4f88-8ae7-fb0c3915d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log2(x):\n",
    "    return int(np.log2(x))\n",
    "\n",
    "\n",
    "# we use different batch size for different resolution, so larger image size\n",
    "# could fit into GPU memory. The keys is image resolution in log2\n",
    "batch_sizes = {2: 16, 3: 16, 4: 16, 5: 16, 6: 16, 7: 8, 8: 4, 9: 2, 10: 1}\n",
    "# We adjust the train step accordingly\n",
    "train_step_ratio = {k: batch_sizes[2] / v for k, v in batch_sizes.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3e5e8ef-5c91-451c-98a1-6ba50bdadcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1330 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "ds_train = keras.preprocessing.image_dataset_from_directory(\n",
    "    \"c:/Users/admin/Documents/Datasets/chinese_calligraphy/\", label_mode=None, image_size=(256, 256), batch_size=32\n",
    ")\n",
    "ds_train = ds_train.map(lambda x: x / 255.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ec2022b-0d3a-4b05-8308-2d297c114e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(res, image):\n",
    "    # only donwsampling, so use nearest neighbor that is faster to run\n",
    "    image = tf.image.resize(\n",
    "        image, (res, res), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\n",
    "    )\n",
    "    image = tf.cast(image, tf.float32) / 127.5 - 1.0\n",
    "    return image\n",
    "\n",
    "\n",
    "def create_dataloader(res):\n",
    "    batch_size = batch_sizes[log2(res)]\n",
    "    # NOTE: we unbatch the dataset so we can `batch()` it again with the `drop_remainder=True` option\n",
    "    # since the model only supports a single batch size\n",
    "    dl = ds_train.map(partial(resize_image, res), num_parallel_calls=tf.data.AUTOTUNE).unbatch()\n",
    "    dl = dl.shuffle(200).batch(batch_size, drop_remainder=True).prefetch(1).repeat()\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f2c71ac-7f53-4b42-962a-ddfaf4fd912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, log2_res, fname=\"\"):\n",
    "    scales = {2: 0.5, 3: 1, 4: 2, 5: 3, 6: 4, 7: 5, 8: 6, 9: 7, 10: 8}\n",
    "    scale = scales[log2_res]\n",
    "\n",
    "    grid_col = min(images.shape[0], int(32 // scale))\n",
    "    grid_row = 1\n",
    "\n",
    "    f, axarr = plt.subplots(\n",
    "        grid_row, grid_col, figsize=(grid_col * scale, grid_row * scale)\n",
    "    )\n",
    "\n",
    "    for row in range(grid_row):\n",
    "        ax = axarr if grid_row == 1 else axarr[row]\n",
    "        for col in range(grid_col):\n",
    "            ax[col].imshow(images[row * grid_col + col])\n",
    "            ax[col].axis(\"off\")\n",
    "    plt.show()\n",
    "    if fname:\n",
    "        f.savefig(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa197e48-0722-4ebb-b447-9927ff6df1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fade_in(alpha, a, b):\n",
    "    return alpha * a + (1.0 - alpha) * b\n",
    "\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return -tf.reduce_mean(y_true * y_pred)\n",
    "\n",
    "\n",
    "def pixel_norm(x, epsilon=1e-8):\n",
    "    return x / tf.math.sqrt(tf.reduce_mean(x ** 2, axis=-1, keepdims=True) + epsilon)\n",
    "\n",
    "\n",
    "def minibatch_std(input_tensor, epsilon=1e-8):\n",
    "    n, h, w, c = tf.shape(input_tensor)\n",
    "    group_size = tf.minimum(4, n)\n",
    "    x = tf.reshape(input_tensor, [group_size, -1, h, w, c])\n",
    "    group_mean, group_var = tf.nn.moments(x, axes=(0), keepdims=False)\n",
    "    group_std = tf.sqrt(group_var + epsilon)\n",
    "    avg_std = tf.reduce_mean(group_std, axis=[1, 2, 3], keepdims=True)\n",
    "    x = tf.tile(avg_std, [group_size, h, w, 1])\n",
    "    return tf.concat([input_tensor, x], axis=-1)\n",
    "\n",
    "\n",
    "class EqualizedConv(layers.Layer):\n",
    "    def __init__(self, out_channels, kernel=3, gain=2, **kwargs):\n",
    "        super(EqualizedConv, self).__init__(**kwargs)\n",
    "        self.kernel = kernel\n",
    "        self.out_channels = out_channels\n",
    "        self.gain = gain\n",
    "        self.pad = kernel != 1\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.in_channels = input_shape[-1]\n",
    "        initializer = keras.initializers.RandomNormal(mean=0.0, stddev=1.0)\n",
    "        self.w = self.add_weight(\n",
    "            shape=[self.kernel, self.kernel, self.in_channels, self.out_channels],\n",
    "            initializer=initializer,\n",
    "            trainable=True,\n",
    "            name=\"kernel\",\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.out_channels,), initializer=\"zeros\", trainable=True, name=\"bias\"\n",
    "        )\n",
    "        fan_in = self.kernel * self.kernel * self.in_channels\n",
    "        self.scale = tf.sqrt(self.gain / fan_in)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.pad:\n",
    "            x = tf.pad(inputs, [[0, 0], [1, 1], [1, 1], [0, 0]], mode=\"REFLECT\")\n",
    "        else:\n",
    "            x = inputs\n",
    "        output = (\n",
    "            tf.nn.conv2d(x, self.scale * self.w, strides=1, padding=\"VALID\") + self.b\n",
    "        )\n",
    "        return output\n",
    "\n",
    "\n",
    "class EqualizedDense(layers.Layer):\n",
    "    def __init__(self, units, gain=2, learning_rate_multiplier=1, **kwargs):\n",
    "        super(EqualizedDense, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.gain = gain\n",
    "        self.learning_rate_multiplier = learning_rate_multiplier\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.in_channels = input_shape[-1]\n",
    "        initializer = keras.initializers.RandomNormal(\n",
    "            mean=0.0, stddev=1.0 / self.learning_rate_multiplier\n",
    "        )\n",
    "        self.w = self.add_weight(\n",
    "            shape=[self.in_channels, self.units],\n",
    "            initializer=initializer,\n",
    "            trainable=True,\n",
    "            name=\"kernel\",\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"zeros\", trainable=True, name=\"bias\"\n",
    "        )\n",
    "        fan_in = self.in_channels\n",
    "        self.scale = tf.sqrt(self.gain / fan_in)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        output = tf.add(tf.matmul(inputs, self.scale * self.w), self.b)\n",
    "        return output * self.learning_rate_multiplier\n",
    "\n",
    "\n",
    "class AddNoise(layers.Layer):\n",
    "    def build(self, input_shape):\n",
    "        n, h, w, c = input_shape[0]\n",
    "        initializer = keras.initializers.RandomNormal(mean=0.0, stddev=1.0)\n",
    "        self.b = self.add_weight(\n",
    "            shape=[1, 1, 1, c], initializer=initializer, trainable=True, name=\"kernel\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, noise = inputs\n",
    "        output = x + self.b * noise\n",
    "        return output\n",
    "\n",
    "\n",
    "class AdaIN(layers.Layer):\n",
    "    def __init__(self, gain=1, **kwargs):\n",
    "        super(AdaIN, self).__init__(**kwargs)\n",
    "        self.gain = gain\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        x_shape = input_shapes[0]\n",
    "        w_shape = input_shapes[1]\n",
    "\n",
    "        self.w_channels = w_shape[-1]\n",
    "        self.x_channels = x_shape[-1]\n",
    "\n",
    "        self.dense_1 = EqualizedDense(self.x_channels, gain=1)\n",
    "        self.dense_2 = EqualizedDense(self.x_channels, gain=1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, w = inputs\n",
    "        ys = tf.reshape(self.dense_1(w), (-1, 1, 1, self.x_channels))\n",
    "        yb = tf.reshape(self.dense_2(w), (-1, 1, 1, self.x_channels))\n",
    "        return ys * x + yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73d37331-a674-4c63-b90c-4f6df00098a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mapping(num_stages, input_shape=512):\n",
    "    z = layers.Input(shape=(input_shape))\n",
    "    w = pixel_norm(z)\n",
    "    for i in range(8):\n",
    "        w = EqualizedDense(512, learning_rate_multiplier=0.01)(w)\n",
    "        w = layers.LeakyReLU(0.2)(w)\n",
    "    w = tf.tile(tf.expand_dims(w, 1), (1, num_stages, 1))\n",
    "    return keras.Model(z, w, name=\"mapping\")\n",
    "\n",
    "\n",
    "class Generator:\n",
    "    def __init__(self, start_res_log2, target_res_log2):\n",
    "        self.start_res_log2 = start_res_log2\n",
    "        self.target_res_log2 = target_res_log2\n",
    "        self.num_stages = target_res_log2 - start_res_log2 + 1\n",
    "        # list of generator blocks at increasing resolution\n",
    "        self.g_blocks = []\n",
    "        # list of layers to convert g_block activation to RGB\n",
    "        self.to_rgb = []\n",
    "        # list of noise input of different resolutions into g_blocks\n",
    "        self.noise_inputs = []\n",
    "        # filter size to use at each stage, keys are log2(resolution)\n",
    "        self.filter_nums = {\n",
    "            0: 512,\n",
    "            1: 512,\n",
    "            2: 512,  # 4x4\n",
    "            3: 512,  # 8x8\n",
    "            4: 512,  # 16x16\n",
    "            5: 512,  # 32x32\n",
    "            6: 256,  # 64x64\n",
    "            7: 128,  # 128x128\n",
    "            8: 64,  # 256x256\n",
    "            9: 32,  # 512x512\n",
    "            10: 16,\n",
    "        }  # 1024x1024\n",
    "\n",
    "        start_res = 2 ** start_res_log2\n",
    "        self.input_shape = (start_res, start_res, self.filter_nums[start_res_log2])\n",
    "        self.g_input = layers.Input(self.input_shape, name=\"generator_input\")\n",
    "\n",
    "        for i in range(start_res_log2, target_res_log2 + 1):\n",
    "            filter_num = self.filter_nums[i]\n",
    "            res = 2 ** i\n",
    "            self.noise_inputs.append(\n",
    "                layers.Input(shape=(res, res, 1), name=f\"noise_{res}x{res}\")\n",
    "            )\n",
    "            to_rgb = Sequential(\n",
    "                [\n",
    "                    layers.InputLayer(input_shape=(res, res, filter_num)),\n",
    "                    EqualizedConv(3, 1, gain=1),\n",
    "                ],\n",
    "                name=f\"to_rgb_{res}x{res}\",\n",
    "            )\n",
    "            self.to_rgb.append(to_rgb)\n",
    "            is_base = i == self.start_res_log2\n",
    "            if is_base:\n",
    "                input_shape = (res, res, self.filter_nums[i - 1])\n",
    "            else:\n",
    "                input_shape = (2 ** (i - 1), 2 ** (i - 1), self.filter_nums[i - 1])\n",
    "            g_block = self.build_block(\n",
    "                filter_num, res=res, input_shape=input_shape, is_base=is_base\n",
    "            )\n",
    "            self.g_blocks.append(g_block)\n",
    "\n",
    "    def build_block(self, filter_num, res, input_shape, is_base):\n",
    "        input_tensor = layers.Input(shape=input_shape, name=f\"g_{res}\")\n",
    "        noise = layers.Input(shape=(res, res, 1), name=f\"noise_{res}\")\n",
    "        w = layers.Input(shape=512)\n",
    "        x = input_tensor\n",
    "\n",
    "        if not is_base:\n",
    "            x = layers.UpSampling2D((2, 2))(x)\n",
    "            x = EqualizedConv(filter_num, 3)(x)\n",
    "\n",
    "        x = AddNoise()([x, noise])\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = AdaIN()([x, w])\n",
    "\n",
    "        x = EqualizedConv(filter_num, 3)(x)\n",
    "        x = AddNoise()([x, noise])\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = AdaIN()([x, w])\n",
    "        return keras.Model([input_tensor, w, noise], x, name=f\"genblock_{res}x{res}\")\n",
    "\n",
    "    def grow(self, res_log2):\n",
    "        res = 2 ** res_log2\n",
    "\n",
    "        num_stages = res_log2 - self.start_res_log2 + 1\n",
    "        w = layers.Input(shape=(self.num_stages, 512), name=\"w\")\n",
    "\n",
    "        alpha = layers.Input(shape=(1), name=\"g_alpha\")\n",
    "        x = self.g_blocks[0]([self.g_input, w[:, 0], self.noise_inputs[0]])\n",
    "\n",
    "        if num_stages == 1:\n",
    "            rgb = self.to_rgb[0](x)\n",
    "        else:\n",
    "            for i in range(1, num_stages - 1):\n",
    "\n",
    "                x = self.g_blocks[i]([x, w[:, i], self.noise_inputs[i]])\n",
    "\n",
    "            old_rgb = self.to_rgb[num_stages - 2](x)\n",
    "            old_rgb = layers.UpSampling2D((2, 2))(old_rgb)\n",
    "\n",
    "            i = num_stages - 1\n",
    "            x = self.g_blocks[i]([x, w[:, i], self.noise_inputs[i]])\n",
    "\n",
    "            new_rgb = self.to_rgb[i](x)\n",
    "\n",
    "            rgb = fade_in(alpha[0], new_rgb, old_rgb)\n",
    "\n",
    "        return keras.Model(\n",
    "            [self.g_input, w, self.noise_inputs, alpha],\n",
    "            rgb,\n",
    "            name=f\"generator_{res}_x_{res}\",\n",
    "        )\n",
    "\n",
    "\n",
    "class Discriminator:\n",
    "    def __init__(self, start_res_log2, target_res_log2):\n",
    "        self.start_res_log2 = start_res_log2\n",
    "        self.target_res_log2 = target_res_log2\n",
    "        self.num_stages = target_res_log2 - start_res_log2 + 1\n",
    "        # filter size to use at each stage, keys are log2(resolution)\n",
    "        self.filter_nums = {\n",
    "            0: 512,\n",
    "            1: 512,\n",
    "            2: 512,  # 4x4\n",
    "            3: 512,  # 8x8\n",
    "            4: 512,  # 16x16\n",
    "            5: 512,  # 32x32\n",
    "            6: 256,  # 64x64\n",
    "            7: 128,  # 128x128\n",
    "            8: 64,  # 256x256\n",
    "            9: 32,  # 512x512\n",
    "            10: 16,\n",
    "        }  # 1024x1024\n",
    "        # list of discriminator blocks at increasing resolution\n",
    "        self.d_blocks = []\n",
    "        # list of layers to convert RGB into activation for d_blocks inputs\n",
    "        self.from_rgb = []\n",
    "\n",
    "        for res_log2 in range(self.start_res_log2, self.target_res_log2 + 1):\n",
    "            res = 2 ** res_log2\n",
    "            filter_num = self.filter_nums[res_log2]\n",
    "            from_rgb = Sequential(\n",
    "                [\n",
    "                    layers.InputLayer(\n",
    "                        input_shape=(res, res, 3), name=f\"from_rgb_input_{res}\"\n",
    "                    ),\n",
    "                    EqualizedConv(filter_num, 1),\n",
    "                    layers.LeakyReLU(0.2),\n",
    "                ],\n",
    "                name=f\"from_rgb_{res}\",\n",
    "            )\n",
    "\n",
    "            self.from_rgb.append(from_rgb)\n",
    "\n",
    "            input_shape = (res, res, filter_num)\n",
    "            if len(self.d_blocks) == 0:\n",
    "                d_block = self.build_base(filter_num, res)\n",
    "            else:\n",
    "                d_block = self.build_block(\n",
    "                    filter_num, self.filter_nums[res_log2 - 1], res\n",
    "                )\n",
    "\n",
    "            self.d_blocks.append(d_block)\n",
    "\n",
    "    def build_base(self, filter_num, res):\n",
    "        input_tensor = layers.Input(shape=(res, res, filter_num), name=f\"d_{res}\")\n",
    "        x = minibatch_std(input_tensor)\n",
    "        x = EqualizedConv(filter_num, 3)(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = EqualizedDense(filter_num)(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = EqualizedDense(1)(x)\n",
    "        return keras.Model(input_tensor, x, name=f\"d_{res}\")\n",
    "\n",
    "    def build_block(self, filter_num_1, filter_num_2, res):\n",
    "        input_tensor = layers.Input(shape=(res, res, filter_num_1), name=f\"d_{res}\")\n",
    "        x = EqualizedConv(filter_num_1, 3)(input_tensor)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = EqualizedConv(filter_num_2)(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = layers.AveragePooling2D((2, 2))(x)\n",
    "        return keras.Model(input_tensor, x, name=f\"d_{res}\")\n",
    "\n",
    "    def grow(self, res_log2):\n",
    "        res = 2 ** res_log2\n",
    "        idx = res_log2 - self.start_res_log2\n",
    "        alpha = layers.Input(shape=(1), name=\"d_alpha\")\n",
    "        input_image = layers.Input(shape=(res, res, 3), name=\"input_image\")\n",
    "        x = self.from_rgb[idx](input_image)\n",
    "        x = self.d_blocks[idx](x)\n",
    "        if idx > 0:\n",
    "            idx -= 1\n",
    "            downsized_image = layers.AveragePooling2D((2, 2))(input_image)\n",
    "            y = self.from_rgb[idx](downsized_image)\n",
    "            x = fade_in(alpha[0], x, y)\n",
    "\n",
    "            for i in range(idx, -1, -1):\n",
    "                x = self.d_blocks[i](x)\n",
    "        return keras.Model([input_image, alpha], x, name=f\"discriminator_{res}_x_{res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ae652d0-208a-4f7c-8fb0-edb59ac67fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleGAN(tf.keras.Model):\n",
    "    def __init__(self, z_dim=512, target_res=64, start_res=4):\n",
    "        super(StyleGAN, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.target_res_log2 = log2(target_res)\n",
    "        self.start_res_log2 = log2(start_res)\n",
    "        self.current_res_log2 = self.target_res_log2\n",
    "        self.num_stages = self.target_res_log2 - self.start_res_log2 + 1\n",
    "\n",
    "        self.alpha = tf.Variable(1.0, dtype=tf.float32, trainable=False, name=\"alpha\")\n",
    "\n",
    "        self.mapping = Mapping(num_stages=self.num_stages)\n",
    "        self.d_builder = Discriminator(self.start_res_log2, self.target_res_log2)\n",
    "        self.g_builder = Generator(self.start_res_log2, self.target_res_log2)\n",
    "        self.g_input_shape = self.g_builder.input_shape\n",
    "\n",
    "        self.phase = None\n",
    "        self.train_step_counter = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
    "\n",
    "        self.loss_weights = {\"gradient_penalty\": 10, \"drift\": 0.001}\n",
    "\n",
    "    def grow_model(self, res):\n",
    "        tf.keras.backend.clear_session()\n",
    "        res_log2 = log2(res)\n",
    "        self.generator = self.g_builder.grow(res_log2)\n",
    "        self.discriminator = self.d_builder.grow(res_log2)\n",
    "        self.current_res_log2 = res_log2\n",
    "        print(f\"\\nModel resolution:{res}x{res}\")\n",
    "\n",
    "    def compile(\n",
    "        self, steps_per_epoch, phase, res, d_optimizer, g_optimizer, *args, **kwargs\n",
    "    ):\n",
    "        self.loss_weights = kwargs.pop(\"loss_weights\", self.loss_weights)\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        if res != 2 ** self.current_res_log2:\n",
    "            self.grow_model(res)\n",
    "            self.d_optimizer = d_optimizer\n",
    "            self.g_optimizer = g_optimizer\n",
    "\n",
    "        self.train_step_counter.assign(0)\n",
    "        self.phase = phase\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "        super(StyleGAN, self).compile(*args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    def generate_noise(self, batch_size):\n",
    "        noise = [\n",
    "            tf.random.normal((batch_size, 2 ** res, 2 ** res, 1))\n",
    "            for res in range(self.start_res_log2, self.target_res_log2 + 1)\n",
    "        ]\n",
    "        return noise\n",
    "\n",
    "    def gradient_loss(self, grad):\n",
    "        loss = tf.square(grad)\n",
    "        loss = tf.reduce_sum(loss, axis=tf.range(1, tf.size(tf.shape(loss))))\n",
    "        loss = tf.sqrt(loss)\n",
    "        loss = tf.reduce_mean(tf.square(loss - 1))\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "\n",
    "        self.train_step_counter.assign_add(1)\n",
    "\n",
    "        if self.phase == \"TRANSITION\":\n",
    "            self.alpha.assign(\n",
    "                tf.cast(self.train_step_counter / self.steps_per_epoch, tf.float32)\n",
    "            )\n",
    "        elif self.phase == \"STABLE\":\n",
    "            self.alpha.assign(1.0)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        alpha = tf.expand_dims(self.alpha, 0)\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        real_labels = tf.ones(batch_size)\n",
    "        fake_labels = -tf.ones(batch_size)\n",
    "\n",
    "        z = tf.random.normal((batch_size, self.z_dim))\n",
    "        const_input = tf.ones(tuple([batch_size] + list(self.g_input_shape)))\n",
    "        noise = self.generate_noise(batch_size)\n",
    "\n",
    "        # generator\n",
    "        with tf.GradientTape() as g_tape:\n",
    "            w = self.mapping(z)\n",
    "            fake_images = self.generator([const_input, w, noise, alpha])\n",
    "            pred_fake = self.discriminator([fake_images, alpha])\n",
    "            g_loss = wasserstein_loss(real_labels, pred_fake)\n",
    "\n",
    "            trainable_weights = (\n",
    "                self.mapping.trainable_weights + self.generator.trainable_weights\n",
    "            )\n",
    "            gradients = g_tape.gradient(g_loss, trainable_weights)\n",
    "            self.g_optimizer.apply_gradients(zip(gradients, trainable_weights))\n",
    "\n",
    "        # discriminator\n",
    "        with tf.GradientTape() as gradient_tape, tf.GradientTape() as total_tape:\n",
    "            # forward pass\n",
    "            pred_fake = self.discriminator([fake_images, alpha])\n",
    "            pred_real = self.discriminator([real_images, alpha])\n",
    "\n",
    "            epsilon = tf.random.uniform((batch_size, 1, 1, 1))\n",
    "            interpolates = epsilon * real_images + (1 - epsilon) * fake_images\n",
    "            gradient_tape.watch(interpolates)\n",
    "            pred_fake_grad = self.discriminator([interpolates, alpha])\n",
    "\n",
    "            # calculate losses\n",
    "            loss_fake = wasserstein_loss(fake_labels, pred_fake)\n",
    "            loss_real = wasserstein_loss(real_labels, pred_real)\n",
    "            loss_fake_grad = wasserstein_loss(fake_labels, pred_fake_grad)\n",
    "\n",
    "            # gradient penalty\n",
    "            gradients_fake = gradient_tape.gradient(loss_fake_grad, [interpolates])\n",
    "            gradient_penalty = self.loss_weights[\n",
    "                \"gradient_penalty\"\n",
    "            ] * self.gradient_loss(gradients_fake)\n",
    "\n",
    "            # drift loss\n",
    "            all_pred = tf.concat([pred_fake, pred_real], axis=0)\n",
    "            drift_loss = self.loss_weights[\"drift\"] * tf.reduce_mean(all_pred ** 2)\n",
    "\n",
    "            d_loss = loss_fake + loss_real + gradient_penalty + drift_loss\n",
    "\n",
    "            gradients = total_tape.gradient(\n",
    "                d_loss, self.discriminator.trainable_weights\n",
    "            )\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(gradients, self.discriminator.trainable_weights)\n",
    "            )\n",
    "\n",
    "        # Update metrics\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        return {\n",
    "            \"d_loss\": self.d_loss_metric.result(),\n",
    "            \"g_loss\": self.g_loss_metric.result(),\n",
    "        }\n",
    "\n",
    "    def call(self, inputs: dict()):\n",
    "        style_code = inputs.get(\"style_code\", None)\n",
    "        z = inputs.get(\"z\", None)\n",
    "        noise = inputs.get(\"noise\", None)\n",
    "        batch_size = inputs.get(\"batch_size\", 1)\n",
    "        alpha = inputs.get(\"alpha\", 1.0)\n",
    "        alpha = tf.expand_dims(alpha, 0)\n",
    "        if style_code is None:\n",
    "            if z is None:\n",
    "                z = tf.random.normal((batch_size, self.z_dim))\n",
    "            style_code = self.mapping(z)\n",
    "\n",
    "        if noise is None:\n",
    "            noise = self.generate_noise(batch_size)\n",
    "\n",
    "        # self.alpha.assign(alpha)\n",
    "\n",
    "        const_input = tf.ones(tuple([batch_size] + list(self.g_input_shape)))\n",
    "        images = self.generator([const_input, style_code, noise, alpha])\n",
    "        images = np.clip((images * 0.5 + 0.5) * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fc175b7-7c56-4be8-8d9d-d748b26546c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_RES = 16\n",
    "TARGET_RES = 256\n",
    "\n",
    "style_gan = StyleGAN(start_res=START_RES, target_res=TARGET_RES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e835bf6b-823f-4b5d-9f87-bb9cd7130b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    start_res=START_RES,\n",
    "    target_res=TARGET_RES,\n",
    "    steps_per_epoch=5000,\n",
    "    display_images=True,\n",
    "):\n",
    "    opt_cfg = {\"learning_rate\": 1e-3, \"beta_1\": 0.0, \"beta_2\": 0.99, \"epsilon\": 1e-8}\n",
    "\n",
    "    val_batch_size = 16\n",
    "    val_z = tf.random.normal((val_batch_size, style_gan.z_dim))\n",
    "    val_noise = style_gan.generate_noise(val_batch_size)\n",
    "\n",
    "    start_res_log2 = int(np.log2(start_res))\n",
    "    target_res_log2 = int(np.log2(target_res))\n",
    "\n",
    "    for res_log2 in range(start_res_log2, target_res_log2 + 1):\n",
    "        res = 2 ** res_log2\n",
    "        for phase in [\"TRANSITION\", \"STABLE\"]:\n",
    "            if res == start_res and phase == \"TRANSITION\":\n",
    "                continue\n",
    "\n",
    "            train_dl = create_dataloader(res)\n",
    "\n",
    "            steps = int(train_step_ratio[res_log2] * steps_per_epoch)\n",
    "\n",
    "            style_gan.compile(\n",
    "                d_optimizer=tf.keras.optimizers.Adam(**opt_cfg),\n",
    "                g_optimizer=tf.keras.optimizers.Adam(**opt_cfg),\n",
    "                loss_weights={\"gradient_penalty\": 10, \"drift\": 0.001},\n",
    "                steps_per_epoch=steps,\n",
    "                res=res,\n",
    "                phase=phase,\n",
    "                run_eagerly=False,\n",
    "            )\n",
    "\n",
    "            prefix = f\"res_{res}x{res}_{style_gan.phase}\"\n",
    "\n",
    "            ckpt_cb = keras.callbacks.ModelCheckpoint(\n",
    "                f\"checkpoints/stylegan_{res}x{res}.ckpt\",\n",
    "                save_weights_only=True,\n",
    "                verbose=0,\n",
    "            )\n",
    "            print(phase)\n",
    "            style_gan.fit(\n",
    "                train_dl, epochs=1, steps_per_epoch=steps, callbacks=[ckpt_cb]\n",
    "            )\n",
    "\n",
    "            if display_images:\n",
    "                images = style_gan({\"z\": val_z, \"noise\": val_noise, \"alpha\": 1.0})\n",
    "                plot_images(images, res_log2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01053694-e09b-4e6f-9918-d30f877abe9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model resolution:16x16\n",
      "STABLE\n",
      "10000/10000 [==============================] - 667s 66ms/step - d_loss: -10.6221 - g_loss: 7.5794\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 3200x200 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model resolution:32x32\n",
      "TRANSITION\n",
      "   27/10000 [..............................] - ETA: 28:57 - d_loss: -2.1156 - g_loss: 1.9074"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_res\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_res\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [10], line 44\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(start_res, target_res, steps_per_epoch, display_images)\u001b[0m\n\u001b[0;32m     38\u001b[0m ckpt_cb \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints/stylegan_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     40\u001b[0m     save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     42\u001b[0m )\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(phase)\n\u001b[1;32m---> 44\u001b[0m \u001b[43mstyle_gan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mckpt_cb\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m display_images:\n\u001b[0;32m     49\u001b[0m     images \u001b[38;5;241m=\u001b[39m style_gan({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m\"\u001b[39m: val_z, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnoise\u001b[39m\u001b[38;5;124m\"\u001b[39m: val_noise, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1.0\u001b[39m})\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1570\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1568\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[0;32m   1569\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1570\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:470\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 470\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:317\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:340\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    343\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:388\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    387\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 388\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:1081\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1081\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:1157\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m--> 635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 628\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(start_res=16, target_res=256, steps_per_epoch=10000, display_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589b0fef-9106-4847-ba4b-a5c984e871e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_gan.grow_model(256)\n",
    "# style_gan.load_weights(os.path.join(\"pretrained/stylegan_128x128.ckpt\"))\n",
    "style_gan.load_weights('./checkpoints/stylegan_256x256.ckpt')\n",
    "\n",
    "tf.random.set_seed(196)\n",
    "batch_size = 2\n",
    "z = tf.random.normal((batch_size, style_gan.z_dim))\n",
    "w = style_gan.mapping(z)\n",
    "noise = style_gan.generate_noise(batch_size=batch_size)\n",
    "images = style_gan({\"style_code\": w, \"noise\": noise, \"alpha\": 1.0})\n",
    "plot_images(images, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2723dbf8-1df2-4ca3-af58-db816146f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.4\n",
    "w_mix = np.expand_dims(alpha * w[0] + (1 - alpha) * w[1], 0)\n",
    "noise_a = [np.expand_dims(n[0], 0) for n in noise]\n",
    "mix_images = style_gan({\"style_code\": w_mix, \"noise\": noise_a})\n",
    "image_row = np.hstack([images[0], images[1], mix_images[0]])\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.imshow(image_row)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e317d57a-2a8d-4e2c-aeee-c0f7437b626f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
